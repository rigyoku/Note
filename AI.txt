1.	python中**表示乘方 2**3=8
2.	python2中整数除整数得到整数,而在python3中整数除整数得到的是浮点数
3.	python使用type(xx)查看数据类型
4.	python的数组提供了切片(slicing)方法 arr[start:end] 注意包含开始位不包含结束位.开始省略为从头开始,结束省略位到最后一位[包含最后一位].结束位为负数意味着从后往前数几位,比如[:-1]意味着从头到倒数第二位[包含]
5.	python中字典即为键值对形式,可以用a['b']取值也可a.get('b', default)取值.
6.	python的布尔运算符是 and or not
7.	python中类的__init__为构造方法,生成实例时执行一次
8.	numpy中进行对应元素运算时需要保证元素个数一致不然报错. 也可以数组每个元素和标量运算,叫做广播.
	通过数组.shape可以得到数组形状,ndim可以得到数组维度. 多维数组元素个数相同时也可以进行对应元素运算,也可以作标量广播.
	数组维度不同时,会根据广播性质来复制扩充维度进行对应运算.
	数组的访问除了下标和遍历,还可以.flatten()转成一位数组来访问. 下标除了普通数字,还可以是数组形式.
	数组和比较运算符计算得出布尔值数组,再将得到的布尔数组作为下标来取值即做了筛选.
9.	matplotlib的pyplot用于绘图, plot方法设置 横纵坐标数组,label线的标签,linestyle线的类型,比如'--'表示虚线,xlabel为x的轴标签,ylabel为y轴标签,legend方法来显示线的标签,title设置图像标题,show来显示整体图像.
	注意设置图像内容要在show之前,每一个show对应一张图,关闭一个图来显示下一个图.每次show会清空之前设置.
	matplotlib提供了图片读取和显示方法. image.imread得到img,然后赋值给plot.imshow(img) 最后plot.show来显示.
10.	感知机接收多个输入,提供一个输出. 每个输入数据有权重,当权重加和大于阈值时,感知机(神经元)被激活.[x1 * w1 + x2 * w2 > c]
11.	机器学习就是确定参数过程,人需要构建模型和提供训练数据.
12.	将感知机的阈值移到左侧,变成 [x1 * w1 + x2 * w2 - c > 0] => [x1 * w1 + x2 * w2 + b > 0] 这样b称作偏置.
13.	单层感知机只是权重 * 数据,为线性空间,无法表示非线性空间如 异或 逻辑.而感知机的叠加可以实现表示非线性空间.
14.	神经网络包含 输入层/中间层[隐藏层]/输出层
15.	感知机根据偏置和权重得出的结果进行判断的方法,叫做激活函数,如之前例子的>0输出1,否则输出0.
16.	朴素感知机使用阶跃函数激活[即超过阈值改变输出],多层感知机使用sigmoid函数激活[1/(1+e^-x)].
17.	先将numpy的数组进行比较运算得到布尔数组,然后通过astype方法传入np.int得到int型数组完成阶跃函数.等价于np.array(newArr, dtype).
18.	plot可以设置ylim为y轴范围
19.	由于numpy数组的广播机制,可以直接将数组作为参数和标量运算.
20.	神经网络必须使用非线性函数作为激活函数,不然加深层就没意义了,
21.	神经网络早期使用sigmoid作为激活函数,现在使用ReLU函数[Rectified Linear Unit 整流线性单元]作为激活函数.可以使用np.maxinum实现.
22.	np的shape返回的形状数组的顺序是按维度解析的,即第一层括号元素个数,然后第二次括号元素个数. 矩阵为2维数组,也就是 行 列 的顺序.\
	np中数组相乘不能使用*来做乘法,而是使用np.dot函数运算. 矩阵的乘法 即 前面矩阵提供行信息,后面矩阵提供列信息. 前矩阵的行遍历乘后矩阵列遍历, 加和得到该行列的元素, 也就是结果为前面矩阵的行数后面矩阵的列数.
	二维数组 和 一维数组 乘算时, 临近的行/列要相同且运算后消失, 剩下的行/列作为列. 即一维数组理解成shape为(1,x)的二维数组,不过和矩阵乘算时对应元素不匹配则转置再乘,然后结果再转置回来即可.
23.	一般情况下,输出函数选用规则:回归问题用恒等输出原值,二元分类用sigmoid函数,多元分类用softmax函数.
24.	神经网络的惯例是权重变量大写,其他小写.
25.	softmax函数: exp(x) / sum[exp(x)], 即输出层所有是输入都会影响输出,而恒等函数只有当前输入连接到输出.
	注意softmax函数进行的是指数运算,指数得到结果会非常大,超出计算机的精度时会出现溢出现象需要注意. 溢出结果为nan, nan意为 not a number
	可以对其进行优化,将输入值加上一个常量,得到的结果不变[相当于e^(a+1),指数的加法对应为结果的乘法,分子分母都乘倍数则结果不变],这个常量取输入最大值的负数即可.
26.	由于softmax函数使用了指数运算,消耗大量运算量,实际使用时一般省略softmax;而softmax的单调递增,所以不会改变输出的位置,所以分类问题也不会使用softmax.
27.	输出层神经元的数量,在分类问题一般和类别数量一致.
28.	推理处理,即前向传播,使用学习到的参数进行处理得到输出. 而学习数据就是得到权重参数的过程.
29.	MNIST数据集是机器学习的常用数据集.
30.	urllib.request.urlretrieve(文件url,保存路径) 用于下载文件, np.frombuffer(流,数据类型,初始偏移量)用于从数据流读取数组, reshape用于数组变形[-1意味着该维度要根据另一维度计算得出]
	pickle.dump用于对象的序列化写入文件保存. np.zeros(shape)来得到对应形状矩阵,元素为0.
31.	np.max可以取得数组中最大元素,而np.argmax可以取得最大元素的下标,可以传入axis指定取最大值的维度[列是0,行是1]
32.	将数据的值限定在一定范围内叫做正规化. 对神经网络输入的既定转换叫做预处理. 预处理可以提高学习效率和精度, 常做正规化,白化[使均匀分布],正态分布.
33.	打包式数据输入称为批[batch],批处理相比单次处理效率更高,可以将时间由数据传输转移到数据计算中.
34.	识别方式: 1.提取特征量[从输入数据提取本质数据[重要数据]的转换器]然后机器学习. 2.深度学习来完全由机器提取特征量.
35.	深度学习也被称为 端到端学习. 即从输出的原始数据得到输出.
36.	为了正确评价模型的泛化能力,将数据分为训练数据和测试数据.训练数据又称为监督数据. 泛化能力指,未被观察过的数据的处理能力. 只对某个数据集过度拟合的情况称为过拟合.应该避免过拟合.
37.	神经网络的学习通过某个指标表示现在的状态,然后以这个指标为基准,寻找最优权重参数. 该指标被称为损失函数. 一般使用均方误差和交叉熵误差.
38.	均方误差: 1/2 * sum_k((yk-tk)^2) 其中k为维度, yk为k维度下的输出,tk为k维度下的监督数据.
39.	正确解的标签为1,其他为0的表现形式叫做 one-hot.
40.	交叉熵误差: - sum_k(tk * log(yk)) 其中k为维度,yk为k维度下的输出,tk为k维度下的监督数据. 由于在log(0)时得到无穷小后续无法运算,需要对yk加上一个微小值[1e-7].
41.	将损失函数扩大到n份数据, 对n份损失数据求和再除n得到平均损失. 数据量大时候用全部数据计算损失函数不现实,应选出一部分数据作为整体的近似,称为mini-batch学习,小批量数据叫做mini-batch.
42.	np.random.choice(x, y)从x个数中随机抽取y个
43.	多维数组的下标可以是和原数组维度一样长度的个数的数组集合. 每个数组用来取该维度的元素.
44.	对权重参数调整以损失函数为基准,为了使损失函数变得更小,需要对参数求导. 即参数发生变化时损失函数的变化,来作为依据调整参数. 特殊的是导数为0时无法进行调整,因为参数变化不会影响损失函数.
	以损失函数作为指标,可以让参数的变化导数不为0[连续], 而以精准度作为指标,大部分导数为0无法继续调整了[非连续].同理阶跃函数由于大部分时候导数是0,不能作为深度学习的激活函数.
45.	导数就是某个瞬间的变化量. df(x)/dx = lim ([f(x+h) - fx] / h) 即 x增大了一个极小值h, fx => f(x+h) 差值除以极小值得到导数.
	如果使用过小的值来实现,比如1e-50,会造成舍入误差[因为省略后几位来运算造成的误差],用10^-4就够了.
	由于h不可能无线小,所以计算的其实不是x处斜率而是x到x+h处斜率[前向差分],更精确应使用中心差分[(f(x+h) - f(x-h)) / 2h]]
	使用微小值计算叫做微分求导,使用数学表达式推理叫做解析求导. 解析求导没有误差,而微分求导存在误差.
46.	多个变量的导数称为偏导数.偏导数求导时,先将一个变量作为常量,得到只有另一个变量的普通函数再求导.
47.	由全部变量汇总而成的向量称为梯度. 负梯度方向是梯度法中变量的更新方向. 梯度所指方向是各点处函数值降低的方向.
48.	np.zeros_like() 返回一个和参数数组一样形状的全为0的数组.
49.	mpt.gca(projection='3d')[返回axes] 获取3d坐标系的子图. axes.plot3D绘制线图, axes.scatter3D绘制散点图, axes.plot_wireframe绘制线框图. 	
	np.meshgrid()生成网格点坐标矩阵[2维坐标的结果分别是行整体重复,列每个值重复]. 以 [a,b][a,b] 为例,得到结果是 [[a,b][a,b]],[[a,a],[b,b]].使用flatten转成一维数组之后,相当于遍历行列.
	mpt.quiver来绘制箭头[梯度],angles参数表示角度[xy: 从a指向b; uv: 横纵比为1,45°]
50.	函数的极小值,最小值和鞍点的梯度为0. 寻找最小值的梯度法称为梯度下降法,寻找最大值的方法称为梯度上升法. 而在深度学习中寻找的是损失函数的最小值,但是损失函数可以变换符号,这样上升法或者下降法就都一样了.
51.	学习率: 更新量.学习一次应该更新多少. 沿着梯度方向走了多远. 该向量分解到x方向就是 n*(x的导数).
	学习率过大导致跳过了最小值点, 学习率过小导致了更新效率低下. 和权重这样由机器得出参数不同, 学习率由人工设定,称为超参数.
52.	神经网络中的梯度就是 损失函数 关于 权重 的变化率.
53.	np.nditer实现[多维]数组的遍历,通过参数op_flag控制获得的遍历是否是只读对象[readwrite/readonly].flags参数获得索引值[f_index/multi_index].
54.	Python也使用lambda表达式定义匿名函数
55.	由于minibatch的选择是随机的,又称为 随机梯度下降法, 简写是 SGD. 
56.	np.random.randn为高斯分布
57.	epoch表示所有训练数据都被使用过一次需要的更新次数. 实际中,一般先打乱源数据,然后根据batch大小来分批执行,这样所有数据都会被使用. 将每个epoch结果制图,如果训练数据和测试数据都在提高说明在正确学习,
	如果训练数据和测试数据基本重叠,说明没有过拟合.
58.	计算图:将计算过程用图形表示出来. 圆圈表示计算内容[节点],值写在横线上方,表示流到下个节点.
59.	局部计算: 只根据自己相关信息,输出接下来的结果.
60.	链式法则: 如果某个函数由复合函数表示,则该复合函数的导数可以用组成复合函数的各个函数的导数乘积表示[反向传播基于链式法则]. 
	可以理解为y变化需要t变化([y/t导数]*y变化度), 而t变化这么多需要x变化 [t/x导数]*t变化度, 即 y变化度 * y/t导数 * t/x导数 = x变化度, 即 x变化这么多,对y的影响有 y/t导数 * t/x导数这么多.
	即 y/x导数 = y/t导数 * t/x导数
61.	numpy.random.permutation 对多维数组的第一个维度进行乱序排列
62.	仿射变换: 一次线性变换和一次平移.
63.	虽然反向传播计算快,但是复杂容易出错,使用微分法进行check称为梯度确认.
64.	如果梯度方向没有指向最小值,则sgd效率就会很低
65.	momentum: 动量法 v = av - l*d; w = w + v; adagrad: 逐步减少学习率[保存权重平方和]; adam融合了momentum和adagrad方法. 
66.	机器学习7个步骤:收集数据/数据准备/选择模型/训练/评估/参数调整/评估.